---
title: "Exercise Quality Prediction"
date: "30/05/2020"
output: html_document:
        keep_md: TRUE
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE ,tidy = TRUE, comment = NULL)
```
## Overview

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website [here](http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har)
The goal of this project is to predict the manner in which they did the exercise (*classe* variable in the training set).
The training data for this project are available [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv).
The test data are available [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)
The data for this project come from this [source](http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har.).
```{r echo = F, message = F}
library(caret)
library(corrplot)
library(Hmisc)
```
## Getting and Cleaning Data

```{r}
if(!"training" %in% ls())
    training <- read.csv("./pml-training.csv", header = TRUE)
if(!"testing" %in% ls())
    testing <- read.csv("./pml-testing.csv", header = TRUE)
```

The training data set is split 70-30 into a train and test data set.
```{r}
set.seed(2110)
intrain<- createDataPartition(training$classe, p = 0.7, list = FALSE)
train <- training[intrain, ]
test <- training[-intrain, ]
```

Variables that have near zero variance are not going to inpact the prediction much, so they're filtered out. Variables with a vast majority of NA (>75 %) are removed. The first 5 columns are also removed as they're ID information and don't help with predicting. 
```{r}
zero <- nearZeroVar(train)
train <- train[, -zero]
test <- test[ , -zero]

not_nas <- sapply(train, function(i) mean(is.na(i)) < 0.75)
train <- train[ , not_nas]
test <- test[ , not_nas]

train <- train[, - (1:5)]
test <- test[ , -(1:5)]

```

## Analysis of Predictors 

A closer look at the remaining variables by finding the correlation matrix tells me the variables which are correlated with correlation > 0.6 with each other
```{r}
cor_train <- rcorr(as.matrix(train[, -54]))
strong_correlation <- findCorrelation(cor_train$r, cutoff = 0.6)
strong_correlation
corrplot(cor_train$r[strong_correlation, ], type = "lower", method = "color")
```

## Model Fitting 
The first attempt is to ignore the correlation matrix and fit a random forest model to the training data. Then, I predict with test data and compared the predictions using the confusion matrix, to find that we get an accuracy of around 99% with 95% confidence and hence based on the test data, the estimate of out of sample error is around 1%. Cross-validation isn't necessary to get an unbiased estimate of the test set error, which is a nice feature of random forests. R estimates the test set error internally. The final predictions on the testing set is also output. 
```{r}
if(!"fit" %in% ls())
    fit <- randomForest::randomForest(classe ~., data = train)
rfpred <- predict(fit, newdata = test)
confusionMatrix(rfpred, test$classe)
testing_pred <- predict(fit, newdata = testing)
testing_pred
```

Now, I try to improve on an already good accuracy of 99% further. A few of the variables are correlated. So principle component analysis is done to reduce the variables to a fewer number. The cumulative variance of the first 30 principle components itself accounts for 99% of the variation. This can also be easily visualised in a plot.
```{r}
pcomp <- prcomp(train[ , -54], scale = TRUE)
pcomp_var <- pcomp$sdev^2
pcomp_var[1:10]
pcomp_varprop <- pcomp_var/sum(pcomp_var)
sum(pcomp_varprop[1:30])*100
plot(cumsum(pcomp_varprop), xlab = "Principal Component",
     ylab = "Cumulative Proportion of Variance",
     type = "b")
```

Now, I use the first 30 principle components to predict *classe*. I suitably predict the principle components for the test dataset as well using those of the train dataset. A random forest model is then fit, predictions on the test data are made and a confusion matrix is output. A surprising result is that the accuracy at around 97% is lower than before when PCA was not performed. This may be because the data was overfitted, or because only the first 30 components were used. It could also be that random forests is winning the tradeoff between overfitting and removing correlated variables which might be too few, or the cutoff too low at 0.6. Based on my test data, the estimate for out of sample error is around 3%.  
```{r}
pca_train <- data.frame(classe = train$classe, pcomp$x)
pca_train <- pca_train[, 1:30]
if(!"fit_PCA" %in% ls())
    fit_PCA <- randomForest::randomForest(classe ~ ., data = pca_train)
    
pca_test <- predict(pcomp, newdata = test[ ,-54])
pca_test <- pca_test[ , 1:30]

rfpred_PCA <- predict(fit_PCA, newdata = pca_test)
confusionMatrix(rfpred_PCA, test$classe)
```

Similarly, the predictions for the testing dataset are made, output and compared to the one from the previous model. Both turn out to be the same. This is not suprising as only 20 test samples were given and it is possible for them to match.
```{r}
pca_testing <- predict(pcomp, newdata = testing[ , -160])
pca_testing <- pca_testing[ , 1:30]

testing_pred_PCA <- predict(fit_PCA, newdata = pca_testing)
testing_pred_PCA
confusionMatrix(testing_pred_PCA, testing_pred)
```


